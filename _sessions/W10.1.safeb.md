
## Required Readings: RISK, SAFETY, Evaluation & GUARDRAILS 
**Core Component:** Agent Safety Systems - Ensuring Reliable, Ethical, and Secure Operation

Addressing safety, alignment, and ethical considerations in agent deployment.

| Topic | Slide Deck | Previous Semester  |
|-------|------------|-------------------|
| Platform - Model Jailbreaking / Safeguarding | W7.1-team3-jailbreak | [25course](https://qiyanjun.github.io/2025sp-GenAI-overview/) |
| Platform - VLM Jailbreaking / Probing | W7.2-team4-MMJailbreak-garak | [25course](https://qiyanjun.github.io/2025sp-GenAI-overview/) |
| Agent Safety | W10.2-team4-agent-safety | [25course](https://qiyanjun.github.io/2025sp-GenAI-overview/) |
| LLM Evaluating Framework | W3-LLMEvaluation-Team5 | [24course](https://qiyanjun.github.io/2024sp-GenAI-Risk-Benefits/) |
| GenAI Guardrails | W3-Guardrail-Team3 | [24course](https://qiyanjun.github.io/2024sp-GenAI-Risk-Benefits/) |
| Survey: Human Alignment | W4-LLM-Human-Alignment | [24course](https://qiyanjun.github.io/2024sp-GenAI-Risk-Benefits/) |
| Survey: AI Risk Framework | W5-AI-RiskFramework | [24course](https://qiyanjun.github.io/2024sp-GenAI-Risk-Benefits/) |
| FM Copyright Infringement | W5-FM-copyright-infrigement | [24course](https://qiyanjun.github.io/2024sp-GenAI-Risk-Benefits/) |
| FM Privacy Leakage Issues | W6-FM-privacy-leakage | [24course](https://qiyanjun.github.io/2024sp-GenAI-Risk-Benefits/) |
| FM Fairness / Bias Issues | W6-LLM-Bias-Fairness-Team5 | [24course](https://qiyanjun.github.io/2024sp-GenAI-Risk-Benefits/) |
| FM Toxicity / Harmful Outputs | W7-LLM-harm | [24course](https://qiyanjun.github.io/2024sp-GenAI-Risk-Benefits/) |
| LLM Multimodal Harm Responses | W7-multimodal-LLMharm | [24course](https://qiyanjun.github.io/2024sp-GenAI-Risk-Benefits/) |
| More FM Risk / Extra - Agent Guardrailing | W8-Team3-P3-moreRisk.pdf | [25course](https://qiyanjun.github.io/2025sp-GenAI-overview/) |



## More Readings: 


#### Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs
+ Zhao Xu, Fan Liu, Hao Liu
+ [Submitted on 13 Jun 2024 (v1), last revised 6 Nov 2024 (this version, v3)]
+ Although Large Language Models (LLMs) have demonstrated significant capabilities in executing complex tasks in a zero-shot manner, they are susceptible to jailbreak attacks and can be manipulated to produce harmful outputs. Recently, a growing body of research has categorized jailbreak attacks into token-level and prompt-level attacks. However, previous work primarily overlooks the diverse key factors of jailbreak attacks, with most studies concentrating on LLM vulnerabilities and lacking exploration of defense-enhanced LLMs. To address these issues, we introduced JailTrickBench to evaluate the impact of various attack settings on LLM performance and provide a baseline for jailbreak attacks, encouraging the adoption of a standardized evaluation framework. Specifically, we evaluate the eight key factors of implementing jailbreak attacks on LLMs from both target-level and attack-level perspectives. We further conduct seven representative jailbreak attacks on six defense methods across two widely used datasets, encompassing approximately 354 experiments with about 55,000 GPU hours on A800-80G. Our experimental results highlight the need for standardized benchmarking to evaluate these attacks on defense-enhanced LLMs. Our code is available at this https URL.








#### Jailbreak Attacks and Defenses Against Large Language Models: A Survey
+ Sibo Yi, Yule Liu, Zhen Sun, Tianshuo Cong, Xinlei He, Jiaxing Song, Ke Xu, Qi Li
+ [Submitted on 5 Jul 2024 (v1), last revised 30 Aug 2024 (this version, v2)]
+ Large Language Models (LLMs) have performed exceptionally in various text-generative tasks, including question answering, translation, code completion, etc. However, the over-assistance of LLMs has raised the challenge of "jailbreaking", which induces the model to generate malicious responses against the usage policy and society by designing adversarial prompts. With the emergence of jailbreak attack methods exploiting different vulnerabilities in LLMs, the corresponding safety alignment measures are also evolving. In this paper, we propose a comprehensive and detailed taxonomy of jailbreak attack and defense methods. For instance, the attack methods are divided into black-box and white-box attacks based on the transparency of the target model. Meanwhile, we classify defense methods into prompt-level and model-level defenses. Additionally, we further subdivide these attack and defense methods into distinct sub-classes and present a coherent diagram illustrating their relationships. We also conduct an investigation into the current evaluation methods and compare them from different perspectives. Our findings aim to inspire future research and practical implementations in safeguarding LLMs against adversarial attacks. Above all, although jailbreak remains a significant concern within the community, we believe that our work enhances the understanding of this domain and provides a foundation for developing more secure LLMs.




#### Safeguarding Large Language Models: A Survey
+  [Submitted on 3 Jun 2024]
+ Yi Dong, Ronghui Mu, Yanghao Zhang, Siqi Sun, Tianle Zhang, Changshun Wu, Gaojie Jin, Yi Qi, Jinwei Hu, Jie Meng, Saddek Bensalem, Xiaowei Huang
+ In the burgeoning field of Large Language Models (LLMs), developing a robust safety mechanism, colloquially known as "safeguards" or "guardrails", has become imperative to ensure the ethical use of LLMs within prescribed boundaries. This article provides a systematic literature review on the current status of this critical mechanism. It discusses its major challenges and how it can be enhanced into a comprehensive mechanism dealing with ethical issues in various contexts. First, the paper elucidates the current landscape of safeguarding mechanisms that major LLM service providers and the open-source community employ. This is followed by the techniques to evaluate, analyze, and enhance some (un)desirable properties that a guardrail might want to enforce, such as hallucinations, fairness, privacy, and so on. Based on them, we review techniques to circumvent these controls (i.e., attacks), to defend the attacks, and to reinforce the guardrails. While the techniques mentioned above represent the current status and the active research trends, we also discuss several challenges that cannot be easily dealt with by the methods and present our vision on how to implement a comprehensive guardrail through the full consideration of multi-disciplinary approach, neural-symbolic method, and systems development lifecycle.
